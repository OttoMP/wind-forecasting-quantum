{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREVISÃO DA VELOCIDADE DO VENTO A CURTO PRAZO USANDO REDES NEURAIS QUÂNTICAS EM MUCURI PARA 3 HORAS SIMULTÂNEAS, BAHIA\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import backend, optimizers, activations\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
    "\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_tabela(arquivo):\n",
    "    # Carregando dataset\n",
    "    dataset_train = pd.read_csv(arquivo, sep='\\t', header = 0)\n",
    "\n",
    "    # Separando os valores entre dados de entrada e dados a serem preditos \n",
    "    # (X e Y) Utilizado apenas a coluna de velocidade e removido o primeiro\n",
    "    # índice para prever a próxima velocidade\n",
    "    y_train_all = dataset_train[:].drop(dataset_train.index[0])\n",
    "    # Remove a ultima linha do X pois não o predito Y não terá uma linha a mais\n",
    "    X_train_all = dataset_train.iloc[:-3,:]\n",
    "    \n",
    "\n",
    "    y_train_all['1h - Vento'] = y_train_all.iloc[:,4].shift(0)\n",
    "    y_train_all['2h - Vento'] = y_train_all.iloc[:,4].shift(-1)\n",
    "    y_train_all['3h - Vento'] = y_train_all.iloc[:,4].shift(-2)\n",
    "    y_train_all = y_train_all.iloc[:-2,-3:]\n",
    "    \n",
    "    return X_train_all,y_train_all.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X (547, 9)\n",
      "Shape y\n",
      " (547, 3)\n",
      "y dataset:\n",
      " [[12.72608696 12.08111113 11.64722224]\n",
      " [12.08111113 11.64722224 11.06444444]\n",
      " [11.64722224 11.06444444 10.32444445]\n",
      " [11.06444444 10.32444445  9.86277779]\n",
      " [10.32444445  9.86277779  9.59888887]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dia</th>\n",
       "      <th>Mês</th>\n",
       "      <th>Ano</th>\n",
       "      <th>Hora</th>\n",
       "      <th>Velocidade</th>\n",
       "      <th>Direção</th>\n",
       "      <th>Temperatura</th>\n",
       "      <th>Umidade</th>\n",
       "      <th>Pressão</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>2015</td>\n",
       "      <td>14</td>\n",
       "      <td>13.012139</td>\n",
       "      <td>75.105481</td>\n",
       "      <td>27.516129</td>\n",
       "      <td>72.930636</td>\n",
       "      <td>1020.422601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>2015</td>\n",
       "      <td>15</td>\n",
       "      <td>12.726087</td>\n",
       "      <td>68.334332</td>\n",
       "      <td>27.238095</td>\n",
       "      <td>75.212121</td>\n",
       "      <td>1020.394348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>2015</td>\n",
       "      <td>16</td>\n",
       "      <td>12.081111</td>\n",
       "      <td>64.457865</td>\n",
       "      <td>27.105263</td>\n",
       "      <td>75.741379</td>\n",
       "      <td>1020.508333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>2015</td>\n",
       "      <td>17</td>\n",
       "      <td>11.647222</td>\n",
       "      <td>53.842100</td>\n",
       "      <td>26.305556</td>\n",
       "      <td>75.302632</td>\n",
       "      <td>1020.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>2015</td>\n",
       "      <td>18</td>\n",
       "      <td>11.064444</td>\n",
       "      <td>53.945279</td>\n",
       "      <td>25.464286</td>\n",
       "      <td>76.592593</td>\n",
       "      <td>1020.866500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dia  Mês   Ano  Hora  ...    Direção  Temperatura   Umidade       Pressão\n",
       "0   30   11  2015    14  ...  75.105481    27.516129  72.930636  1020.422601\n",
       "1   30   11  2015    15  ...  68.334332    27.238095  75.212121  1020.394348\n",
       "2   30   11  2015    16  ...  64.457865    27.105263  75.741379  1020.508333\n",
       "3   30   11  2015    17  ...  53.842100    26.305556  75.302632  1020.611000\n",
       "4   30   11  2015    18  ...  53.945279    25.464286  76.592593  1020.866500\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'train150_mucuri.txt'\n",
    "\n",
    "X_train_all,y_train_all = carregar_tabela(filename)\n",
    "\n",
    "\n",
    "print(\"Shape X\", X_train_all.shape)\n",
    "print(\"Shape y\\n\", y_train_all.shape)\n",
    "\n",
    "print(\"y dataset:\\n\", y_train_all[:5])\n",
    "\n",
    "X_train_all.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_scaled = scaler_x.fit_transform(X_train_all)\n",
    "\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y_train_scaled, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Quântica "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_layer(n_qubits):\n",
    "    for idx in range(n_qubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "def Data_AngleEmbedding_layer(inputs, n_qubits):\n",
    "    qml.templates.AngleEmbedding(inputs,rotation='Y', wires=range(n_qubits))\n",
    "\n",
    "def RY_layer(w):\n",
    "    print(w.shape)\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "\n",
    "def ROT_layer(w):\n",
    "    for i in range(5):\n",
    "        qml.Rot(*w[i],wires=i)\n",
    "\n",
    "def strong_entangling_layer(nqubits):\n",
    "    qml.CNOT(wires=[0,1])\n",
    "    qml.CNOT(wires=[1,2])\n",
    "    qml.CNOT(wires=[2,3])\n",
    "    qml.CNOT(wires=[3,4])\n",
    "    qml.CNOT(wires=[4,0])\n",
    "    \n",
    "    \n",
    "def entangling_layer(nqubits):\n",
    "    for i in range(0, nqubits - 1, 2): \n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):  \n",
    "        qml.CNOT(wires=[i, i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 5\n",
    "n_layers = 1\n",
    "\n",
    "#dev = qml.device('lightning.qubit', wires=n_qubits)\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "@qml.qnode(dev)\n",
    "def qnode(inputs, weights_1):\n",
    "    H_layer(n_qubits)\n",
    "    Data_AngleEmbedding_layer(inputs, n_qubits)\n",
    "    for k in range(n_layers):\n",
    "        entangling_layer(n_qubits)\n",
    "        ROT_layer(weights_1[k])\n",
    "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_shapes = {\"weights_1\": (n_layers,5,3)}\n",
    "qlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits)\n",
    "Activation=tf.keras.layers.Activation(activations.linear)\n",
    "clayer_2 = tf.keras.layers.Dense(1,kernel_initializer='normal')\n",
    "model = tf.keras.models.Sequential([qlayer,Activation, clayer_2])\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "model.compile(opt, loss=\"mse\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo\n",
    "\n",
    "<img src=\"neural network.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def create_MLP_model(neurons, activation_function, forecast_range=3, optimizer=tf.keras.optimizers.SGD(learning_rate = 0.01)):\n",
    "    entrada = tf.keras.layers.Input(shape=(9,))\n",
    "    networks = []\n",
    "    for i in range(forecast_range):\n",
    "        l = tf.keras.layers.Dense(neurons[0], activation=activation_function)(entrada)\n",
    "        for j in range(len(neurons)):\n",
    "            l = tf.keras.layers.Dense(neurons[j], activation=activation_function)(l)\n",
    "        l = tf.keras.layers.Dense(1, activation=activation_function)(l)\n",
    "        print(l)\n",
    "        networks.append(l)\n",
    "    total = tf.keras.layers.concatenate(networks)\n",
    "    saida = tf.keras.layers.Dense(forecast_range, activation=activation_function)(total)\n",
    "    model = tf.keras.Model(inputs=entrada, outputs=saida)\n",
    "    model.compile(loss=['mse'], optimizer=optimizer, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_MLP_model(neurons=[16,8], activation_function='tanh');\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando o Compilador e Executando o treino\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <th><p align=\"left\">Variável</p></th>\n",
    "    <th><p align=\"left\">Valor</p></th>\n",
    "<tr>\n",
    "    <td><p align=\"left\">Loss</p></td>\n",
    "    <td><p align=\"left\">MSE (Mean Square Error)</p></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><p align=\"left\">Optimizer</p></td>\n",
    "    <td><p align=\"left\">SGD</p></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><p align=\"left\">Metrics</p></td>\n",
    "    <td><p align=\"left\">MAE (Mean Absolute Error)</p></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><p align=\"left\">Epochs</p></td>\n",
    "    <td><p align=\"left\">1000</p></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><p align=\"left\">Batch_Size</p></td>\n",
    "    <td><p align=\"left\">16</p></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><p align=\"left\">Verbose</p></td>\n",
    "    <td><p align=\"left\">2 (Exibir apenas o Epoch com o Loss e a Metric)</p></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><p align=\"left\">tx (Taxa de Aprendizado)</p></td>\n",
    "    <td><p align=\"left\">0.01</p></td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit retorna histórico do modelo\n",
    "history_model = model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "393/393 [==============================] - 1s 2ms/step - loss: 0.1371 - mae: 0.2878 - val_loss: 0.0567 - val_mae: 0.1914 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0870 - mae: 0.2343 - val_loss: 0.0507 - val_mae: 0.1817 - lr: 0.0100\n",
      "Epoch 3/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0842 - mae: 0.2278 - val_loss: 0.0435 - val_mae: 0.1659 - lr: 0.0100\n",
      "Epoch 4/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0812 - mae: 0.2251 - val_loss: 0.0430 - val_mae: 0.1665 - lr: 0.0100\n",
      "Epoch 5/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0805 - mae: 0.2225 - val_loss: 0.0509 - val_mae: 0.1828 - lr: 0.0100\n",
      "Epoch 6/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0795 - mae: 0.2208 - val_loss: 0.0443 - val_mae: 0.1709 - lr: 0.0100\n",
      "Epoch 7/30\n",
      "364/393 [==========================>...] - ETA: 0s - loss: 0.0787 - mae: 0.2177\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "393/393 [==============================] - 1s 2ms/step - loss: 0.0778 - mae: 0.2169 - val_loss: 0.0431 - val_mae: 0.1672 - lr: 0.0100\n",
      "Epoch 8/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0747 - mae: 0.2129 - val_loss: 0.0402 - val_mae: 0.1582 - lr: 1.0000e-03\n",
      "Epoch 9/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0738 - mae: 0.2121 - val_loss: 0.0402 - val_mae: 0.1587 - lr: 1.0000e-03\n",
      "Epoch 10/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0736 - mae: 0.2117 - val_loss: 0.0410 - val_mae: 0.1577 - lr: 1.0000e-03\n",
      "Epoch 11/30\n",
      "392/393 [============================>.] - ETA: 0s - loss: 0.0731 - mae: 0.2111\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0731 - mae: 0.2112 - val_loss: 0.0407 - val_mae: 0.1613 - lr: 1.0000e-03\n",
      "Epoch 12/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0737 - mae: 0.2107 - val_loss: 0.0402 - val_mae: 0.1593 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "393/393 [==============================] - 1s 2ms/step - loss: 0.0732 - mae: 0.2105 - val_loss: 0.0401 - val_mae: 0.1584 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "390/393 [============================>.] - ETA: 0s - loss: 0.0726 - mae: 0.2098\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0731 - mae: 0.2105 - val_loss: 0.0401 - val_mae: 0.1579 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1579 - lr: 1.0000e-05\n",
      "Epoch 16/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1579 - lr: 1.0000e-05\n",
      "Epoch 17/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1579 - lr: 1.0000e-05\n",
      "Epoch 18/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1579 - lr: 1.0000e-05\n",
      "Epoch 19/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1578 - lr: 1.0000e-05\n",
      "Epoch 20/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1578 - lr: 1.0000e-05\n",
      "Epoch 21/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1578 - lr: 1.0000e-05\n",
      "Epoch 22/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1578 - lr: 1.0000e-05\n",
      "Epoch 23/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1578 - lr: 1.0000e-05\n",
      "Epoch 24/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1578 - lr: 1.0000e-05\n",
      "Epoch 25/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1577 - lr: 1.0000e-05\n",
      "Epoch 26/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1577 - lr: 1.0000e-05\n",
      "Epoch 27/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1577 - lr: 1.0000e-05\n",
      "Epoch 28/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1577 - lr: 1.0000e-05\n",
      "Epoch 29/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1577 - lr: 1.0000e-05\n",
      "Epoch 30/30\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0730 - mae: 0.2106 - val_loss: 0.0401 - val_mae: 0.1577 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "es=EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
    "re=ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='min', min_lr=0.00001)\n",
    "fitting = model.fit(X_train, y_train, epochs=30, batch_size=1, validation_split=0.1, callbacks=[re], verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando gráfico de Loss por Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(14,5), dpi=320, facecolor='w', edgecolor='k')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(history.history['loss'], label=\"Loss/Epoch\")\n",
    "    plt.plot(history.history['val_loss'], label=\"Val Loss/Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste\n",
    "### Carregando o Teste e executando a comparação entre original e predito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'prev150_mucuri.txt'\n",
    "X_test_all,y_test_all = carregar_tabela(test)\n",
    "X_test_scaled = scaler.transform(X_test_all)\n",
    "y_test_scaled = scaler_y.transform(y_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predito = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predito_normal = scaler_y.inverse_transform(predito)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predict=model.predict(X_test_scaled,verbose=1)\n",
    "#y_predict=y_predict.T\n",
    "#y_predict=y_predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.07086386471545147\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test_scaled, y_predict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando o intervalo de confiança do erro de predição do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_interval(model, X_val, Y_val, X_test, Y_test, y_test_pred, p_value):\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_error = np.abs(Y_val - y_val_pred)\n",
    "    error_quantile=np.ndarray((1,Y_val.shape[1]));\n",
    "    for i in range(Y_val.shape[1]):\n",
    "        error_quantile[0,i] = np.quantile(y_val_error[:,i], q=p_value, interpolation='higher')\n",
    "        \n",
    "    y_test_interval_pred_left=np.ndarray(y_test_pred.shape);\n",
    "    y_test_interval_pred_right=np.ndarray(y_test_pred.shape);\n",
    "    \n",
    "    for i in range(y_test_pred.shape[1]):\n",
    "        y_test_interval_pred_left[:,i] = y_test_pred[:,i] - error_quantile[0,i]\n",
    "        y_test_interval_pred_right[:,i] = y_test_pred[:,i] + error_quantile[0,i]\n",
    "    return error_quantile, y_test_interval_pred_left, y_test_interval_pred_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_left_right_error_interval(model, y_scaler, y_test_pred):\n",
    "    error, error_left, error_right = get_error_interval(model, X_val, y_val, X_test_scaled, y_test_scaled, y_test_pred, 0.95)\n",
    "    \n",
    "    error_left_normal = y_scaler.inverse_transform(error_left)\n",
    "    error_right_normal = y_scaler.inverse_transform(error_right)\n",
    "\n",
    "    mean_error_normal=np.ndarray((1,y_test_all.shape[1]));\n",
    "    mean_error_left_normal=np.ndarray((1,y_test_all.shape[1]));\n",
    "    mean_error_right_normal=np.ndarray((1,y_test_all.shape[1]));\n",
    "    mean_predictions=np.ndarray((1,y_test_pred.shape[1]));\n",
    "\n",
    "    for i in range(y_test_all.shape[1]):\n",
    "        mean_error_left_normal[0,i] = np.mean(error_left_normal[:,i])\n",
    "        mean_error_right_normal[0,i] = np.mean(error_right_normal[:,i])\n",
    "        mean_predictions[0,i]=np.mean(y_test_pred[:,i])\n",
    "\n",
    "    mean_error_normal=(mean_error_right_normal-mean_error_left_normal)/2\n",
    "    return mean_predictions, mean_error_normal, mean_error_left_normal, mean_error_right_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions, mean_error_normal, mean_error_left_normal, mean_error_right_normal = get_mean_left_right_error_interval(\n",
    "    model, scaler_y, predito_normal)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotando os gráficos de predição versus observado no teste, junto com as métricas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Estatística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_of_2(y_true, y_pred):\n",
    "    min_ = 0.5\n",
    "    max_ = 2.0\n",
    "\n",
    "    tensor_true = tf.constant(y_true)\n",
    "    tensor_true = tf.cast(tensor_true, tf.float32)\n",
    "    tensor_pred = tf.constant(y_pred)\n",
    "    tensor_pred = tf.cast(tensor_pred, tf.float32)\n",
    "\n",
    "    division = tf.divide(tensor_pred, tensor_true)\n",
    "\n",
    "    greater_min = tf.greater_equal(division, min_)\n",
    "    less_max = tf.less_equal(division, max_)\n",
    "\n",
    "    res = tf.equal(greater_min, less_max)\n",
    "    res = tf.cast(res, tf.float32)\n",
    "\n",
    "    return backend.get_value(tf.reduce_mean(res))\n",
    "\n",
    "\n",
    "def allmetrics(original,predito):\n",
    "    r_value = 0\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(original, predito)\n",
    "    mse = mean_squared_error(original, predito)\n",
    "    mae = mean_absolute_error(original, predito)\n",
    "    rr = r2_score(original,predito)\n",
    "    pea = stats.pearsonr(original, predito)\n",
    "    fat = factor_of_2(original,predito)\n",
    "    nmse = mse/stats.tvar(original)\n",
    "    rmse = sqrt(mse)\n",
    "    nrmse = rmse/stats.tstd(original)\n",
    "    return mae,mse,nmse,r_value,rr,fat,rmse,nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_prediction_versus_observed(model):\n",
    "    valores = []\n",
    "    for i in range(y_test_all.shape[1]):\n",
    "        mae,mse,nmse,r_value,rr,fat,rmse,nrmse = allmetrics(y_test_all[:,i],predito_normal[:,i])\n",
    "        valores.append([str(i+1)+\" hora\",mae,mse,nmse,rmse,nrmse,r_value,rr,fat,mean_error_normal[0,i],mean_error_left_normal[0,i],mean_predictions[0,i],mean_error_right_normal[0,i]])\n",
    "        print(\"MAE:\",mae)\n",
    "        print(\"MSE:\",mse)\n",
    "        print(\"NMSE:\",nmse)\n",
    "        print(\"RMSE:\",rmse)\n",
    "        print(\"NRMSE:\",nrmse)\n",
    "        print(\"R:\",r_value)\n",
    "        print(\"R²:\",rr)\n",
    "        print(\"Fator de 2:\",fat)\n",
    "\n",
    "        plt.figure(figsize=(20,5), dpi=320, facecolor='w', edgecolor='k')\n",
    "        plt.title(\"Previsão do vento para \"+str(i+1)+\" hora(s) à frente\")\n",
    "        plt.xlabel(\"Amostras\")\n",
    "        plt.ylabel(\"Velocidade do Vento (m/s)\")\n",
    "        plt.plot(predito_normal[:,i], label=\"Predito\", color='blue')\n",
    "        plt.fill_between(range(predito_normal.shape[0]), predito_normal[:,i]-mean_error_normal[0,i], predito_normal[:,i]+mean_error_normal[0,i], color='blue', alpha=0.05)\n",
    "        plt.plot(y_test_all[:,i], label=\"Original\", color='orange')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    erros = pd.DataFrame(valores)\n",
    "    erros.columns = ['Horas à frente','MAE','MSE','NMSE','RMSE','NRMSE','R','R²','Fator de 2', 'error interval (+/-)', 'left limit', 'mean', 'right limit']\n",
    "    erros = erros.set_index('Horas à frente')\n",
    "    erros.loc['Média'] = erros.mean()\n",
    "    return erros;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erros_pd=get_plot_prediction_versus_observed(model)\n",
    "erros_pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para verificar se as predições de dois modelos diferentes sobre o mesmo dataset possui diferença estatística, i.e. são equivalentes ou diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "# sources: \n",
    "#   https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html\n",
    "#   https://pythonfordatascienceorg.wordpress.com/wilcoxon-sign-ranked-test-python/\n",
    "#   https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/\n",
    "# em resumo: o teste de Wilcoxon signed-rank testa a hipótese nula de que duas amostras pareadas relacionadas vêm da mesma distribuição. Ela é não paramétrica.\n",
    "def verify_distribution_wilcoxtest(data1, data2, p_H0):\n",
    "    stat, p = wilcoxon(data1, data2)\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "    if p > p_H0:\n",
    "        print('Same distribution (fail to reject H0)')\n",
    "    else:\n",
    "        print('Different distribution (reject H0)')\n",
    "    return stat, p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando se há diferença estatística entre o observado e o predito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_distribution_wilcoxtest(y_test_all[:,0],predito_normal[:,0], 0.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando novo modelo e verificando se suas predições têm ou não diferença estatística entre o modelo anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria e ajusta outra rede MLP para comparar com a anterior\n",
    "model2 = create_MLP_model(neurons=[32,16], activation_function='relu', optimizer=tf.keras.optimizers.RMSprop());\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model2 = model2.fit(X_train, y_train, epochs=50, batch_size=64, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predito2 = model2.predict(X_test_scaled)\n",
    "predito2_normal = scaler_y.inverse_transform(predito2)\n",
    "plot_history(history_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_distribution_wilcoxtest(predito_normal[:,0],predito2_normal[:,0], 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_distribution_wilcoxtest(predito_normal[:,1],predito2_normal[:,1], 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_distribution_wilcoxtest(predito_normal[:,2],predito2_normal[:,2], 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erros2_pd=get_plot_prediction_versus_observed(model2)\n",
    "erros2_pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
